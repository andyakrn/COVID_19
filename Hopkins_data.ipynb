{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>\n",
    "    Loading In Data\n",
    "    </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "import os\n",
    "import pickle\n",
    "import base64\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"github.txt\") as myfile:\n",
    "    firstNlines=myfile.readlines()[0:2]\n",
    "myfile.close()\n",
    "g = Github(firstNlines[0].strip(), firstNlines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo=g.get_repo('CSSEGISandData/COVID-19')\n",
    "contents = repo.get_contents(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha_for_tag(repository, tag):\n",
    "    branches = repository.get_branches()\n",
    "    matched_branches = [match for match in branches if match.name == tag]\n",
    "    if matched_branches:\n",
    "        return matched_branches[0].commit.sha\n",
    "\n",
    "    tags = repository.get_tags()\n",
    "    matched_tags = [match for match in tags if match.name == tag]\n",
    "    if not matched_tags:\n",
    "        raise ValueError('No Tag or Branch exists with that name')\n",
    "    return matched_tags[0].commit.sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_directory(repository, sha, server_path, local_path='data_csse/'):\n",
    "    contents = repository.get_contents(server_path, ref=sha)\n",
    "    if not os.path.exists(local_path):\n",
    "        os.makedirs(local_path)\n",
    "    for content in contents:\n",
    "        if content.type == 'dir':\n",
    "            download_directory(repository, sha, content.path)\n",
    "        else:\n",
    "            try:\n",
    "                path = content.path\n",
    "                file_content = repository.get_contents(path, ref=sha)\n",
    "                file_data = base64.b64decode(file_content.content).decode('ascii')\n",
    "                file_out = open(local_path+content.name, \"w\")\n",
    "                file_out.write(local_path+file_data)\n",
    "                file_out.close()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sha = get_sha_for_tag(repo, 'master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_directory(repo, sha, 'csse_covid_19_data/csse_covid_19_time_series/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = 'data_csse/'\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_non_zero_file(fpath):\n",
    "    return os.path.isfile(fpath) and os.path.getsize(fpath) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_files = []\n",
    "for file in onlyfiles:\n",
    "    if is_non_zero_file(mypath + file) and file[:14]=='time_series_19':\n",
    "        timeseries_files.append(mypath + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df, name):\n",
    "    df.drop(columns=['data_csse/Province/State', 'Lat', 'Long'], inplace=True)\n",
    "    df = df.groupby(['Country/Region']).agg('sum')\n",
    "    df = df.transpose().reset_index()\n",
    "    country_list = list(df.columns)[1:]\n",
    "    df = pd.melt(df, id_vars='index', value_vars=country_list)\n",
    "    df = df.rename(columns={'index':'Date', 'value':name})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = pd.read_csv('data_csse/time_series_19-covid-Confirmed.csv')\n",
    "deaths_df = pd.read_csv('data_csse/time_series_19-covid-Deaths.csv')\n",
    "recovered_df = pd.read_csv('data_csse/time_series_19-covid-Recovered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = preprocess_df(confirmed_df, 'Confirmed')\n",
    "deaths_df = preprocess_df(deaths_df, 'Deaths')\n",
    "recovered_df = preprocess_df(recovered_df, 'Recovered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>\n",
    "    Preprocessing Data\n",
    "    </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confirmed_and_deaths = pd.merge(confirmed_df, deaths_df, how='inner', on=['Date', 'Country/Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.merge(confirmed_and_deaths, recovered_df, how='inner', on=['Date', 'Country/Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Active'] = grouped_df['Confirmed']-grouped_df['Deaths']-grouped_df['Recovered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Datetime'] = grouped_df['Date'].apply(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.read_csv('pop_df4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df['Country/Region'] = pop_df['Country/Region'].apply(lambda x: x.replace('Mainland China', 'China'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Mainland China', 'China')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Hong Kong SAR', 'Hong Kong')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].replace(['Korea, South', 'Republic of Korea'], 'South Korea')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('United Kingdom', 'UK')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].replace(['Taiwan*', 'Taipei and environs'], 'Taiwan')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Iran (Islamic Republic of)', 'Iran')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Viet Nam', 'Vietnam')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Macao SAR', 'Macau')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Republic of Ireland', 'Ireland')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Czechia', 'Czech Republic')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('occupied Palestinian territory', 'Palestine')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Russian Federation', 'Russia')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace(' Azerbaijan', 'Azerbaijan')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Holy See', 'Vatican City')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Republic of Moldova', 'Moldova')\n",
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('Saint Martin', 'St. Martin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = pd.merge(grouped_df, pop_df, on='Country/Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Confirmed Cases Per 1M'] = grouped_df['Confirmed']*1000/grouped_df['PopTotal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['Country/Region'] = grouped_df['Country/Region'].str.replace('US', 'United States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>\n",
    "    Export Dataframe\n",
    "    </center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('COVID_Hopkins_df.pickle', 'wb')\n",
    "pickle.dump(grouped_df, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
